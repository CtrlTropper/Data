# %pip install langchain langchain-community langchain-openai
# !pip install underthesea

import fitz  # PyMuPDF
import pytesseract
from PIL import Image, ImageEnhance, ImageFilter
import io
from datetime import datetime
import numpy as np
from sentence_transformers import SentenceTransformer
import pickle
import os
import re
from underthesea import sent_tokenize
from transformers import AutoTokenizer
import faiss
import torch

# Kiá»ƒm tra GPU
print(f"Sá»‘ lÆ°á»£ng GPU: {torch.cuda.device_count()}")
for i in range(torch.cuda.device_count()):
    print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    print(f"  Tá»•ng VRAM: {torch.cuda.get_device_properties(i).total_memory / 1024 / 1024**2:.2f} GB")

device = torch.device("cuda:0")
print(f"\nÄang sá»­ dá»¥ng: {torch.cuda.get_device_name(device)}")

# Load model embedding
model_path = "D:/Vian/Step2_Embeding_and_VectorDB/models/multilingual_e5_large"
model = SentenceTransformer(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# ÄÆ°á»ng dáº«n input folder chá»©a cÃ¡c PDF (má»›i thÃªm Ä‘á»ƒ xá»­ lÃ½ hÃ ng loáº¡t)
input_folder = "D:/Vian/Data/documents"  # Thay báº±ng folder chá»©a nhiá»u PDF cybersecurity
output_dir = "./results"
os.makedirs(output_dir, exist_ok=True)  # Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³

all_faiss_path = os.path.join(output_dir, "all_faiss.index")
all_pickle_path = os.path.join(output_dir, "all_embeddings.pkl")

def is_pdf_embedded(path):
    """
    Kiá»ƒm tra xem file PDF Ä‘Ã£ Ä‘Æ°á»£c embedding hay chÆ°a dá»±a trÃªn file pickle chung.
    """
    if not os.path.exists(all_pickle_path):
        return False
    pdf_name = os.path.splitext(os.path.basename(path))[0]
    with open(all_pickle_path, 'rb') as f:
        all_data = pickle.load(f)
    existing_pdf_names = {entry['pdf_name'] for entry in all_data}
    return pdf_name in existing_pdf_names

def preprocess_image(img):
    """
    Tiá»n xá»­ lÃ½ áº£nh Ä‘á»ƒ cáº£i thiá»‡n OCR.
    """
    if img.mode != 'L':
        img = img.convert('L')
    enhancer = ImageEnhance.Contrast(img)
    img = enhancer.enhance(1.5)
    img = img.filter(ImageFilter.SHARPEN)
    return img

def ocr_pdf_to_text(pdf_path, output_dir):
    """
    OCR file PDF thÃ nh text vÃ  lÆ°u vÃ o folder riÃªng.
    """
    try:
        print(f"ğŸ“– Äang OCR file: {pdf_path}")
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        full_text = ""
        ocr_config = r'--oem 3 --psm 6 -l vie'
        for page_num in range(total_pages):
            print(f"ğŸ”„ Xá»­ lÃ½ trang {page_num + 1}/{total_pages}...")
            page = doc.load_page(page_num)
            matrix = fitz.Matrix(2.5, 2.5)
            pix = page.get_pixmap(matrix=matrix)
            img_data = pix.tobytes("png")
            img = Image.open(io.BytesIO(img_data))
            img = preprocess_image(img)
            page_text = pytesseract.image_to_string(img, config=ocr_config)
            full_text += page_text.strip()
        doc.close()
        print(f"âœ… HoÃ n thÃ nh OCR {total_pages} trang")
        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]
        pdf_folder = os.path.join(output_dir, pdf_name)
        os.makedirs(pdf_folder, exist_ok=True)
        output_path = os.path.join(pdf_folder, f"{pdf_name}_ocr.txt")
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(full_text)
        print(f"ğŸ“„ Káº¿t quáº£ OCR lÆ°u vÃ o: {output_path}")
        return full_text
    except Exception as e:
        print(f"âŒ Lá»—i OCR cho file {pdf_path}: {e}")
        return None

def clean_text(text, pdf_path, output_dir):
    """
    LÃ m sáº¡ch text OCR vÃ  lÆ°u vÃ o folder riÃªng.
    """
    text = re.sub(r'[^\w\s.,;:()\[\]?!\"\'\-â€“â€”â€¦Â°%â€°â‰¥â‰¤â†’â†â‰ =+/*<>\n\r]', '', text)
    text = re.sub(r'-\n', '', text)
    text = re.sub(r'\n(?=\w)', ' ', text)
    text = re.sub(r'\.{3,}', '...', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r' *\n *', '\n', text)
    cleaned_text = text.strip()
    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]
    pdf_folder = os.path.join(output_dir, pdf_name)
    os.makedirs(pdf_folder, exist_ok=True)
    output_path = os.path.join(pdf_folder, f"{pdf_name}_clean.txt")
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(cleaned_text)
    print(f"ğŸ“„ Káº¿t quáº£ clean lÆ°u vÃ o: {output_path}")
    return cleaned_text

def split_sections(text):
    """
    TÃ¡ch text thÃ nh sections theo tiÃªu Ä‘á».
    """
    sections = re.split(r'\n(?=(?:[IVXLCDM]+\.)|(?:\d+\.)|(?:[a-z]\)))', text)
    return [s.strip() for s in sections if s.strip()]

def split_text_to_chunks_vi_tokenized_with_section(text, chunk_size=512, overlap=50):
    """
    Chia text thÃ nh chunks dá»±a trÃªn token, giá»¯ cáº¥u trÃºc section.
    """
    sections = split_sections(text)
    all_chunks = []
    for section in sections:
        sentences = sent_tokenize(section)
        current_chunk = []
        current_tokens = 0
        for sentence in sentences:
            num_tokens = len(tokenizer.tokenize(sentence))
            if current_tokens + num_tokens > chunk_size:
                chunk_text = '\n'.join(current_chunk).strip()
                all_chunks.append(chunk_text)
                overlap_chunk = []
                total = 0
                for s in reversed(current_chunk):
                    toks = len(tokenizer.tokenize(s))
                    if total + toks > overlap:
                        break
                    overlap_chunk.insert(0, s)
                    total += toks
                current_chunk = overlap_chunk + [sentence]
                current_tokens = total + num_tokens
            else:
                current_chunk.append(sentence)
                current_tokens += num_tokens
        if current_chunk:
            all_chunks.append(' '.join(current_chunk).strip())
    return all_chunks

def create_embeddings(chunks):
    """
    Táº¡o embeddings cho chunks.
    """
    try:
        print(f"ğŸ”„ Táº¡o embeddings cho {len(chunks)} chunks...")
        embeddings = model.encode(chunks, show_progress_bar=True)
        print(f"âœ… HoÃ n thÃ nh táº¡o embeddings")
        return embeddings
    except Exception as e:
        print(f"âŒ Lá»—i táº¡o embeddings: {e}")
        return None

def save_embeddings(chunks, embeddings, pdf_path, output_dir):
    """
    LÆ°u embeddings vÃ o folder riÃªng vÃ  cáº­p nháº­t file chung.
    """
    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]
    pdf_folder = os.path.join(output_dir, pdf_name)
    os.makedirs(pdf_folder, exist_ok=True)

    data = {
        'pdf_name': pdf_name,
        'chunks': chunks,
        'embeddings': embeddings,
        'created_at': datetime.now().isoformat()
    }

    # LÆ°u embeddings pickle riÃªng
    pickle_path = os.path.join(pdf_folder, f"{pdf_name}_embeddings.pkl")
    with open(pickle_path, 'wb') as f:
        pickle.dump(data, f)

    # LÆ°u chunks text riÃªng
    chunks_path = os.path.join(pdf_folder, f"{pdf_name}_chunks.txt")
    with open(chunks_path, 'w', encoding='utf-8') as f:
        f.write(f"CHUNKS Tá»ª FILE: {pdf_name}.pdf\n")
        f.write(f"Táº¡o lÃºc: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Tá»•ng sá»‘ chunks: {len(chunks)}\n")
        f.write("=" * 60 + "\n\n")
        for i, chunk in enumerate(chunks, 1):
            f.write(f"CHUNK {i}:\n")
            f.write("-" * 30 + "\n")
            f.write(chunk + "\n")
            f.write("-" * 30 + "\n\n")

    # LÆ°u info embeddings riÃªng
    embedding_info_path = os.path.join(pdf_folder, f"{pdf_name}_embedding_info.txt")
    with open(embedding_info_path, 'w', encoding='utf-8') as f:
        f.write(f"THÃ”NG TIN EMBEDDINGS: {pdf_name}.pdf\n")
        f.write(f"Táº¡o lÃºc: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"ğŸ“Š THá»NG KÃŠ:\n")
        f.write(f"- Tá»•ng sá»‘ chunks: {len(chunks)}\n")
        f.write(f"- KÃ­ch thÆ°á»›c embeddings: {embeddings.shape}\n")
        f.write(f"- Kiá»ƒu dá»¯ liá»‡u: {embeddings.dtype}\n")
        f.write(f"- KÃ­ch thÆ°á»›c má»—i vector: {embeddings.shape[1]} dimensions\n\n")
        f.write(f"ğŸ“ PREVIEW EMBEDDINGS (5 chunks Ä‘áº§u):\n")
        f.write("-" * 50 + "\n")
        for i in range(min(5, len(chunks))):
            f.write(f"\nCHUNK {i+1}:\n")
            f.write(f"Text: {chunks[i][:100]}...\n")
            f.write(f"Embedding vector (10 giÃ¡ trá»‹ Ä‘áº§u): {embeddings[i][:10].tolist()}\n")
            f.write(f"Vector norm: {np.linalg.norm(embeddings[i]):.4f}\n")
            f.write("-" * 30 + "\n")

    # LÆ°u FAISS index riÃªng
    index_path = os.path.join(pdf_folder, f"{pdf_name}_faiss.index")
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings.astype(np.float32))
    faiss.write_index(index, index_path)

    # Cáº­p nháº­t FAISS chung
    if os.path.exists(all_faiss_path):
        index_all = faiss.read_index(all_faiss_path)
    else:
        index_all = faiss.IndexFlatL2(dim)
    index_all.add(embeddings.astype(np.float32))
    faiss.write_index(index_all, all_faiss_path)

    # Cáº­p nháº­t pickle chung
    if os.path.exists(all_pickle_path):
        with open(all_pickle_path, 'rb') as f:
            all_data = pickle.load(f)
    else:
        all_data = []
    all_data.append(data)
    with open(all_pickle_path, 'wb') as f:
        pickle.dump(all_data, f)

    print(f"ğŸ’¾ ÄÃ£ lÆ°u embeddings riÃªng: {pickle_path}")
    print(f"ğŸ“„ ÄÃ£ lÆ°u chunks riÃªng: {chunks_path}")
    print(f"ğŸ“Š ÄÃ£ lÆ°u info embeddings riÃªng: {embedding_info_path}")
    print(f"ğŸ“Œ ÄÃ£ lÆ°u FAISS index riÃªng: {index_path}")
    print(f"ğŸ” Cáº­p nháº­t FAISS chung: {all_faiss_path}")
    print(f"ğŸ“¦ Cáº­p nháº­t pickle chung: {all_pickle_path}")

    return pickle_path, index_path

# Quy trÃ¬nh chÃ­nh: Embedding táº¥t cáº£ PDF trong input_folder (chá»©c nÄƒng má»›i)
pdf_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.pdf')]
print(f"ğŸ” TÃ¬m tháº¥y {len(pdf_files)} file PDF trong folder: {input_folder}")

for pdf_file in pdf_files:
    pdf_path = os.path.join(input_folder, pdf_file)
    if is_pdf_embedded(pdf_path):
        print(f"ğŸ“Œ File {pdf_file} Ä‘Ã£ Ä‘Æ°á»£c embedding trÆ°á»›c Ä‘Ã³, bá» qua.")
        continue
    
    raw_text = ocr_pdf_to_text(pdf_path, output_dir)
    if raw_text:
        print(f"ğŸ§¹ LÃ m sáº¡ch text cho {pdf_file}...")
        cleaned_text = clean_text(raw_text, pdf_path, output_dir)
        print(f"âœ‚ï¸ Chia text thÃ nh chunks cho {pdf_file}...")
        chunks = split_text_to_chunks_vi_tokenized_with_section(cleaned_text)
        print(f"ğŸ“ ÄÃ£ táº¡o {len(chunks)} chunks cho {pdf_file}")
        embeddings = create_embeddings(chunks)
        if embeddings is not None:
            pickle_path, faiss_path = save_embeddings(chunks, embeddings, pdf_path, output_dir)
            print(f"\nğŸ‰ HOÃ€N THÃ€NH embedding cho {pdf_file}!")
            print(f"ğŸ“Š Thá»‘ng kÃª: - Sá»‘ chunks: {len(chunks)} - KÃ­ch thÆ°á»›c embedding: {embeddings.shape}")
            print(f"âœ… ÄÆ°á»ng dáº«n embeddings: {pickle_path}")
            print(f"âœ… ÄÆ°á»ng dáº«n FAISS index: {faiss_path}")

print("\nâœ… ÄÃ£ xá»­ lÃ½ táº¥t cáº£ PDF trong folder. Knowledge base Ä‘Ã£ cáº­p nháº­t!")